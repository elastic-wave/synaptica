services:
  llm-server:
    build:
      context: ..
      dockerfile: build/Dockerfile.jetson
    image: trtllm-jetson
    container_name: llm-server
    command: bash -lc "export TRTLLM_ENGINE_DIR='/workspace/releases/TinyLlama-ctx2048-fp16-kvfp16-mb1-pg128' && python runtime/http_server.py"
    volumes:
      - ../releases:/workspace/releases
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    runtime: nvidia
    ports:
      - "5000:5000"
