# Jetson Orin Nano / aarch64
# Choose a tag matching your flashed JetPack (example shown: r36.4.0 ≈ JetPack 6.1)
FROM nvcr.io/nvidia/l4t-jetpack:r36.2.0

ENV DEBIAN_FRONTEND=noninteractive \
    TZ=UTC \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# ---- System deps (build toolchain + basics) ----
RUN apt-get update && apt-get install -y --no-install-recommends \
    git git-lfs ca-certificates curl \
    build-essential \
    python3 python3-pip python3-dev \
    cmake ninja-build \
    libprotobuf-dev protobuf-compiler \
    && rm -rf /var/lib/apt/lists/* \
    && git lfs install

# Keep pip modern & add a few helpful Python libs
RUN python3 -m pip install --upgrade pip && \
    python3 -m pip install --no-cache-dir wheel packaging numpy tqdm

# ---- Get TensorRT-LLM (Jetson branch) ----
ARG TRTLLM_BRANCH=v0.12.0-jetson
RUN git clone --branch ${TRTLLM_BRANCH} --depth 1 https://github.com/NVIDIA/TensorRT-LLM.git /opt/TensorRT-LLM
WORKDIR /opt/TensorRT-LLM
RUN git submodule update --init --recursive && git lfs pull || true

# (Optional) install Python requirements from repo; ignore if file is minimal on this branch
RUN if [ -f requirements.txt ]; then python3 -m pip install --no-cache-dir -r requirements.txt || true; fi

# ---- Build C++ core + Python bindings WITHOUT NCCL ----
# NOTE: The top-level doesn't have CMakeLists.txt; it's in cpp/
# Key flags:
#   -DENABLE_MULTI_DEVICE=OFF   => keeps NCCL/distributed out
#   -DCMAKE_CUDA_ARCHITECTURES=87  => Orin Nano (Ampere SM 8.7)
#   -DBUILD_PYTHON=ON           => build Python extensions
#   -DBUILD_TESTS=OFF, -DTRTLLM_BUILD_EXAMPLES=OFF => faster, leaner
RUN mkdir -p build && cd build && \
    cmake -G Ninja \
      -DCMAKE_BUILD_TYPE=Release \
      -DBUILD_PYTHON=ON \
      -DBUILD_TESTS=OFF \
      -DTRTLLM_BUILD_EXAMPLES=OFF \
      -DTRTLLM_ENABLE_NVTX=OFF \
      -DENABLE_MULTI_DEVICE=OFF \
      -DCMAKE_CUDA_ARCHITECTURES=87 \
      ../cpp && \
    ninja && \
    echo "✅ TensorRT-LLM build complete."

# Make sure the built shared libs are discoverable at runtime
ENV LD_LIBRARY_PATH=/opt/TensorRT-LLM/build:$LD_LIBRARY_PATH

# Install the Python package in editable mode so `import tensorrt_llm` works
RUN python3 -m pip install --no-cache-dir -e /opt/TensorRT-LLM/python

# Quick sanity checks (won’t fail the build if TRT Python isn’t present in base)
RUN python3 - <<'PY' || true
try:
    import tensorrt as trt
    print("TensorRT Python found:", trt.__version__)
except Exception as e:
    print("Note: TensorRT Python import failed (ok on some bases):", e)
import tensorrt_llm as tllm
print("TensorRT-LLM Python located at:", tllm.__file__)
PY

# Workspace mount point for your models/checkpoints/engines
WORKDIR /workspace

# Friendly banner + interactive shell by default
CMD ["/bin/bash", "-lc", "echo 'TensorRT-LLM (single-GPU, no NCCL) ready. Use trtllm-build with --nccl_plugin disable.' && bash"]
