# Jetson Orin Nano / aarch64
# Choose a tag matching your flashed JetPack (example shown: r36.4.0 ≈ JetPack 6.1)
#FROM nvcr.io/nvidia/l4t-jetpack:r36.2.0
FROM dustynv/l4t-pytorch:r36.4.0

ENV DEBIAN_FRONTEND=noninteractive \
    TZ=UTC \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# ---- System deps (build toolchain + basics) ----
RUN apt-get update && apt-get install -y --no-install-recommends \
    git git-lfs ca-certificates curl \
    build-essential \
    python3 python3-pip python3-dev \
    cmake ninja-build \
    libprotobuf-dev protobuf-compiler \
    && rm -rf /var/lib/apt/lists/* \
    && git lfs install

    # Make PyPI the default; don’t inherit a custom mirror
ENV PIP_INDEX_URL=https://pypi.org/simple
ENV PIP_EXTRA_INDEX_URL=
ENV PIP_DISABLE_PIP_VERSION_CHECK=1 PIP_DEFAULT_TIMEOUT=120

    # General deps come from PyPI
RUN python3 -m pip install --no-cache-dir --retries 5 --timeout 300 \
    pip setuptools wheel packaging numpy tqdm

# --- Torch install (prefer local wheel to avoid flaky index) ---
# (Place the wheel alongside your Dockerfile; it's copied into the build context)
ARG TORCH_WHL_URL=""
COPY torch-2.6.0-cp310-cp310-linux_aarch64.whl /tmp/torch-2.6.0-cp310-cp310-linux_aarch64.whl
RUN if [ -f /tmp/torch-2.6.0-cp310-cp310-linux_aarch64.whl ]; then \
      python3 -m pip install --no-cache-dir --retries 5 --timeout 600 /tmp/torch-2.6.0-cp310-cp310-linux_aarch64.whl; \
    elif [ -n "$TORCH_WHL_URL" ]; then \
      python3 -m pip install --no-cache-dir --retries 5 --timeout 600 "$TORCH_WHL_URL"; \
    else \
      echo 'No local wheel and no TORCH_WHL_URL provided' >&2; exit 1; \
    fi

# ---- Get TensorRT-LLM (Jetson branch) ----
ARG TRTLLM_BRANCH=v0.12.0-jetson
RUN git clone --branch ${TRTLLM_BRANCH} --depth 1 https://github.com/NVIDIA/TensorRT-LLM.git /opt/TensorRT-LLM
WORKDIR /opt/TensorRT-LLM
RUN git submodule update --init --recursive && git lfs pull || true

# (Optional) install Python requirements from repo; ignore if file is minimal on this branch
RUN if [ -f requirements.txt ]; then python3 -m pip install --no-cache-dir -r requirements.txt || true; fi

# pybind11 for both CMake and Python import
RUN apt-get update && apt-get install -y --no-install-recommends \
      pybind11-dev python3-pybind11 \
    && rm -rf /var/lib/apt/lists/*

# (also install the Python wheel so `import pybind11` works in CMake checks)
RUN python3 -m pip install --no-cache-dir pybind11

# Remove old distro pybind11 and install a newer pip version
RUN apt-get update && apt-get remove -y python3-pybind11 pybind11-dev && \
    apt-get install -y --no-install-recommends python3-dev && \
    rm -rf /var/lib/apt/lists/* && \
    python3 -m pip install --no-cache-dir "pybind11>=2.11,<2.13"

    # TensorRT + cuDNN development headers/libs for JP 6.2 (L4T r36.4)
RUN apt-get update && apt-get install -y --no-install-recommends \
    libnvinfer-dev libnvinfer-plugin-dev \
    libnvonnxparsers-dev libnvparsers-dev \
    libcudnn8 libcudnn8-dev \
  && rm -rf /var/lib/apt/lists/*

  # sanity check
RUN ls -l /usr/include/aarch64-linux-gnu/NvInfer.h && \
    ls -l /usr/lib/aarch64-linux-gnu/libnvinfer.so && \
    ls -l /usr/include/aarch64-linux-gnu/cudnn*.h && \
    ls -l /usr/lib/aarch64-linux-gnu/libcudnn*.so

# Build (query pybind11 CMake dir at build time)
RUN rm -rf build && mkdir -p build && cd build && \
    PYBIND11_DIR=$(python3 -c "import pybind11; print(pybind11.get_cmake_dir())") && \
    cmake -S ../cpp -B . \
      -DCMAKE_BUILD_TYPE=Release \
      -DBUILD_PYTHON=ON -DBUILD_TESTS=OFF \
      -DTRTLLM_BUILD_EXAMPLES=OFF -DTRTLLM_ENABLE_NVTX=OFF \
      -DENABLE_MULTI_DEVICE=OFF -DCMAKE_CUDA_ARCHITECTURES=87 \
      -DPYBIND11_FINDPYTHON=OFF \
      -DPython3_EXECUTABLE=/usr/bin/python3 \
      -Dpybind11_DIR="$PYBIND11_DIR" \
      -DTensorRT_DIR=/usr/lib/aarch64-linux-gnu/cmake/TensorRT \
      -DTRT_LIB_DIR=/usr/lib/aarch64-linux-gnu \
      -DTRT_INCLUDE_DIRS=/usr/include/aarch64-linux-gnu \
      -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda && \
    ninja -v && echo "✅ TensorRT-LLM build complete."

# Make sure the built shared libs are discoverable at runtime
ENV LD_LIBRARY_PATH=/opt/TensorRT-LLM/build:$LD_LIBRARY_PATH

# Install the Python package in editable mode so `import tensorrt_llm` works
RUN python3 -m pip install --no-cache-dir -e /opt/TensorRT-LLM/python

# Quick sanity checks (won’t fail the build if TRT Python isn’t present in base)
RUN python3 - <<'PY' || true
try:
    import tensorrt as trt
    print("TensorRT Python found:", trt.__version__)
except Exception as e:
    print("Note: TensorRT Python import failed (ok on some bases):", e)
import tensorrt_llm as tllm
print("TensorRT-LLM Python located at:", tllm.__file__)
PY

# Workspace mount point for your models/checkpoints/engines
WORKDIR /workspace

# Friendly banner + interactive shell by default
CMD ["/bin/bash", "-lc", "echo 'TensorRT-LLM (single-GPU, no NCCL) ready. Use trtllm-build with --nccl_plugin disable.' && bash"]
