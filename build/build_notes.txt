# On the Jetson Orin Nano (with JetPack 6.x), build the image:
# docker build -t trtllm-jetson-nonccl:0.12 --build-arg TRTLLM_BRANCH=v0.12.0-jetson --build-arg TORCH_WHL_URL=https://pypi.jetson-ai-lab.io/jp6/cu126/+f/6cc/6ecfe8a5994fd/torch-2.6.0-cp310-cp310-linux_aarch64.whl -f build/Dockerfile.alt_jetson .

#note: don't pass the url on the build anymore - the file was moved so need to load from local download

docker build --network=host -t trtllm-jetson-nonccl:0.12   --build-arg TRTLLM_BRANCH=v0.12.0-jetson -f build/Dockerfile.alt_jetson .

Note: Donâ€™t activate a Python virtual environment during the Docker build. Docker itself is a virtual environment

# download the whl file directly to local

# download the torch file and access it locally (build will copy from /projects/synaptica folder)
# see https://pypi.jetson-ai-lab.io/jp6/cu126/torch/2.8.0

URL='https://pypi.jetson-ai-lab.io/jp6/cu126/+f/62a/1beee9f2f1470/torch-2.8.0-cp310-cp310-linux_aarch64.whl#sha256=62a1beee9f2f147076a974d2942c90060c12771c94740830327cae705b2595fc'
curl -L --retry 10 --retry-delay 5 --connect-timeout 60 --max-time 0 -o build/torch-2.6.0-cp310-cp310-linux_aarch64.whl "$URL"

# then copy to project root before runing build
cp build/torch-2.6.0-cp310-cp310-linux_aarch64.whl .

# two step build
docker build --network=host -t trtllm-jetson-nonccl:0.12   --build-arg TRTLLM_BRANCH=v0.12.0-jetson -f build/Dockerfile.twostep .

docker build -t trtllm-jetson-multistage:0.12 -f build/Dockerfile.twostep  .

#2step build with full refresh (for when changed configuration/library sources)

docker build --no-cache -t trtllm-jetson-multistage:0.12 -f Dockerfile.twostep .



# Run it with the NVIDIA runtime and host IPC (TRT-LLM recommends --ipc=host):
docker run --rm -it --gpus all --ipc=host \
  -v $PWD:/workspace \
  trtllm-jetson-nonccl:0.12


  #debugging build errors - run from cmmonad prompt with the container (if it built)

  docker run --rm -it trtllm-jetson-nonccl:0.12 /bin/bash
cd /opt/TensorRT-LLM && rm -rf build && mkdir build && cd build

cmake -S ../cpp -B . \
  -DCMAKE_BUILD_TYPE=Release \
  -DBUILD_PYTHON=ON -DBUILD_TESTS=OFF \
  -DTRTLLM_BUILD_EXAMPLES=OFF -DTRTLLM_ENABLE_NVTX=OFF \
  -DENABLE_MULTI_DEVICE=OFF -DCMAKE_CUDA_ARCHITECTURES=87 \
  -DPython3_EXECUTABLE=/usr/bin/python3 \
  -Dpybind11_DIR=$(python3 -c "import pybind11; print(pybind11.get_cmake_dir())") \
  -Wdev --debug-find --trace-expand

